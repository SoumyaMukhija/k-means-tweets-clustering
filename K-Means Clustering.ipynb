{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WsqT6WfUJdea"
      },
      "outputs": [],
      "source": [
        "# Library Imports\n",
        "import pandas as pd\n",
        "import re as regex\n",
        "import requests\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Index from which the actual tweet starts. This will enable us to remove tweet id and timestamp without using regex\n",
        "TWEET_START_INDEX = 50"
      ],
      "metadata": {
        "id": "_Kp6_w8mJgNF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class K_Means_Clustering:\n",
        "    def __init__(self, data_file):\n",
        "        # Get data and split into seperate tweets\n",
        "        self.raw_input = requests.get(data_file)\n",
        "        self.raw_input = self.raw_input.text\n",
        "        self.raw_input = self.raw_input.split('\\n')\n",
        "        \n",
        "        # Loaded Successfully\n",
        "        print(\"Data loaded successfully. Total tweets: \", len(self.raw_input), \"\\n\")\n",
        "\n",
        "    # Helper Functions Start\n",
        "    def get_jaccard_dist(self, tweet_1, tweet_2):\n",
        "        # Take intersection\n",
        "        intersection = len(list(set(tweet_1).intersection(tweet_2)))\n",
        "\n",
        "        # Take union\n",
        "        union = (len(tweet_1) + len(tweet_2)) - intersection\n",
        "        return float(1 - (intersection / union))\n",
        "\n",
        "    def clusterize(self, list_of_list_of_tweets, centroids): \n",
        "        # Dictionary of clusters\n",
        "        dict_of_words = {}\n",
        "\n",
        "        # For tweet_1 and tweet_2, compute their jaccard distance and then add the closest tweets to disctionary\n",
        "        for tweet_1 in list_of_list_of_tweets:\n",
        "            jaccard_dist = []\n",
        "            for tweet_2 in range(len(centroids)):\n",
        "                jaccard_dist.append(self.get_jaccard_dist(centroids[tweet_2], tweet_1))\n",
        "            min_dist = jaccard_dist.index(min(jaccard_dist))\n",
        "            dict_of_words.setdefault(min_dist, [])\n",
        "            dict_of_words[min_dist].append(tweet_1)\n",
        "        return dict_of_words \n",
        "\n",
        "    def calculate_centroid(self, list_of_list_of_tweets):\n",
        "        min_dists = {}\n",
        "        idx = 0\n",
        "        for tweet_1 in list_of_list_of_tweets:\n",
        "            list_of_min_dist = []\n",
        "            for tweet_2 in list_of_list_of_tweets:\n",
        "                dist = self.get_jaccard_dist(tweet_1, tweet_2)\n",
        "                list_of_min_dist.append(dist) \n",
        "            min_dists[idx] = sum(list_of_min_dist)\n",
        "            idx = idx + 1\n",
        "        min_dist_cluster_idx = [(key, value)[0] for key, value in min_dists.items() if value == min(min_dists.values())]\n",
        "        return list_of_list_of_tweets[min_dist_cluster_idx[0]]\n",
        "\n",
        "    def calculate_sse_error(self, centroids, clusters):\n",
        "        sse_error = 0\n",
        "        # print(\"Clusters:\", clusters.keys())\n",
        "        # print(\"Centroids:\", centroid)\n",
        "        for key, value in clusters.items():\n",
        "            for data_point in value:\n",
        "                sse_error += self.get_jaccard_dist(centroids[key], data_point)**2\n",
        "\n",
        "        return sse_error\n",
        "    # Helper Functions End\n",
        "\n",
        "    def pre_process(self):\n",
        "        print(\"Pre-processing the data\\n\")\n",
        "        self.processed_data = []\n",
        "        \n",
        "        for tweet in self.raw_input:\n",
        "            # Remove the tweet id and timestamp\n",
        "            modified_tweet = tweet[TWEET_START_INDEX:]\n",
        "\n",
        "            # Remove any word that starts with @\n",
        "            modified_tweet = regex.sub('@\\S+', \"\", modified_tweet)\n",
        "\n",
        "            # Remove any hastag symbols from words\n",
        "            modified_tweet = regex.sub('#', \"\", modified_tweet)\n",
        "\n",
        "            # Remove any url \n",
        "            modified_tweet = regex.sub('http\\S+', '', modified_tweet)\n",
        "\n",
        "            # Convert every word to lowercase\n",
        "            modified_tweet = modified_tweet.lower()\n",
        "\n",
        "            # Also remove any other symbols since we believe it is introducing noise\n",
        "            modified_tweet = regex.sub(r'[\\'’‘\\\"?,:-]', ' ', modified_tweet)\n",
        "\n",
        "            # Split the tweet into a list of words\n",
        "            modified_tweet = modified_tweet.split()\n",
        "\n",
        "            self.processed_data.append(modified_tweet)\n",
        "        \n",
        "        # Check if pre_process is successful by printing first 30 tweets\n",
        "        for string in self.processed_data[:30]:\n",
        "            print(string)\n",
        "\n",
        "        print(\"\\nPre-processed data successfully\\n\")\n",
        "\n",
        "    def train(self, k):\n",
        "        print(\"\\nTraining stage started\\n\")\n",
        "        centroids = []\n",
        "        centroids_new = []\n",
        "        random_idx = random.sample(range(0, len(self.processed_data) - 1), k)\n",
        "        # print(\"Random: \",random_idx) # Check if random is working fine\n",
        "        for idx in random_idx:\n",
        "            centroids.append(self.processed_data[idx])\n",
        "        centroids_old = centroids\n",
        "        iteration = 1\n",
        "\n",
        "        while True:\n",
        "            centroids_new = []\n",
        "            clusters = self.clusterize(self.processed_data, centroids_old)\n",
        "            for words in clusters:\n",
        "                centroids_new.append(self.calculate_centroid(clusters[words]))\n",
        "            print(\"SSE Error for iteration\", iteration, \":\" + str(self.calculate_sse_error(centroids_new,clusters)))\n",
        "            iteration += 1\n",
        "            if centroids_old == centroids_new:\n",
        "                break\n",
        "            centroids_old = centroids_new\n",
        "            \n",
        "        return centroids_new, clusters\n",
        "        \n",
        "if __name__ == \"__main__\":\n",
        "    # List of hyperparameters and results\n",
        "    values_of_k_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "    size_of_cluster_list = []\n",
        "    sse_error_list = []\n",
        "\n",
        "    # Get the data\n",
        "    KMC = K_Means_Clustering(\"https://raw.githubusercontent.com/Shreyans1602/Machine_Learning_K_Means_Clustering/main/BBC_Health_Dataset.txt\")\n",
        "    \n",
        "    # Pre-process the data\n",
        "    KMC.pre_process()\n",
        "\n",
        "    # Train\n",
        "    for k in values_of_k_list:\n",
        "        centroid, clusters = KMC.train(k)\n",
        "        size_of_clusters = {}\n",
        "        for cluster in clusters:\n",
        "            size_of_clusters[cluster] = len(clusters[cluster])\n",
        "        size_of_cluster_list.append(size_of_clusters)\n",
        "        sse_error_list.append(KMC.calculate_sse_error(centroid, clusters))\n",
        "        print(\"Final SSE Error: \", KMC.calculate_sse_error(centroid, clusters))\n",
        "\n",
        "    # Make a table to print results and export a csv\n",
        "    results_table = pd.DataFrame()\n",
        "    results_table[\"Value_of_K\"] = values_of_k_list\n",
        "    results_table[\"SSE_Error\"] = sse_error_list\n",
        "    results_table[\"Size_of_Clusters\"] = size_of_cluster_list\n",
        "    results_table.index = results_table.index + 1\n",
        "    results_table.to_csv('results.csv')\n",
        "    print(\"\\nPrinting the required output table:\\n\")\n",
        "    print(results_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX5C3dHjJzC5",
        "outputId": "353abccb-157e-4694-e737-e095844f6881"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully. Total tweets:  3929 \n",
            "\n",
            "Pre-processing the data\n",
            "\n",
            "['breast', 'cancer', 'risk', 'test', 'devised']\n",
            "['gp', 'workload', 'harming', 'care', 'bma', 'poll']\n",
            "['short', 'people', 's', 'heart', 'risk', 'greater']\n",
            "['new', 'approach', 'against', 'hiv', 'promising']\n",
            "['coalition', 'undermined', 'nhs', 'doctors']\n",
            "['review', 'of', 'case', 'against', 'nhs', 'manager']\n",
            "['video', 'all', 'day', 'is', 'empty', 'what', 'am', 'i', 'going', 'to', 'do']\n",
            "['video', 'overhaul', 'needed', 'for', 'end', 'of', 'life', 'care']\n",
            "['care', 'for', 'dying', 'needs', 'overhaul']\n",
            "['video', 'nhs', 'labour', 'and', 'tory', 'key', 'policies']\n",
            "['have', 'gp', 'services', 'got', 'worse']\n",
            "['a&amp;e', 'waiting', 'hits', 'new', 'worst', 'level']\n",
            "['parties', 'row', 'over', 'gp', 'opening', 'hours']\n",
            "['why', 'strenuous', 'runs', 'may', 'not', 'be', 'so', 'bad', 'after', 'all']\n",
            "['video', 'health', 'surcharge', 'for', 'non', 'eu', 'patients']\n",
            "['video', 'skin', 'cancer', 'spike', 'from', '60s', 'holidays']\n",
            "['80', '000', 'might', 'die', 'in', 'future', 'outbreak']\n",
            "['skin', 'cancer', 'linked', 'to', 'holiday', 'boom']\n",
            "['public', 'back', 'tax', 'rises', 'to', 'fund', 'nhs']\n",
            "['video', 'welcome', 'to', 'the', 'designer', 'asylum']\n",
            "['video', 'why', 'are', 'we', 'having', 'less', 'sex']\n",
            "['five', 'ideas', 'to', 'transform', 'the', 'nhs']\n",
            "['personal', 'cancer', 'vaccines', 'exciting']\n",
            "['child', 'heart', 'surgery', 'deaths', 'halved']\n",
            "['video', 'miliband', 'cameron', 'failed', 'the', 'nhs']\n",
            "['unsafe', 'food', 'growing', 'global', 'threat']\n",
            "['health', 'highlights']\n",
            "['ambulance', 'progress', 'not', 'fast', 'enough']\n",
            "['children', 's', 'hospital', 'builds', 'sleep', 'app']\n",
            "['drug', 'giant', 'blocks', 'eye', 'treatment']\n",
            "\n",
            "Pre-processed data successfully\n",
            "\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3670.3280567116763\n",
            "SSE Error for iteration 2 :3561.774377503947\n",
            "SSE Error for iteration 3 :3369.793293405396\n",
            "Final SSE Error:  3369.793293405396\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3759.3154155217185\n",
            "SSE Error for iteration 2 :3686.474183611868\n",
            "SSE Error for iteration 3 :3666.623252367153\n",
            "SSE Error for iteration 4 :3204.8471648113236\n",
            "Final SSE Error:  3204.8471648113236\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3689.5940479712567\n",
            "SSE Error for iteration 2 :3463.451842489678\n",
            "SSE Error for iteration 3 :3174.977366096251\n",
            "Final SSE Error:  3174.977366096251\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3802.456608834616\n",
            "SSE Error for iteration 2 :3636.932038798344\n",
            "SSE Error for iteration 3 :3189.5107939974064\n",
            "SSE Error for iteration 4 :3198.3235481608085\n",
            "SSE Error for iteration 5 :3097.8400127092414\n",
            "Final SSE Error:  3097.8400127092414\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3772.7180522846897\n",
            "SSE Error for iteration 2 :3580.345194376453\n",
            "SSE Error for iteration 3 :3095.4498342496613\n",
            "Final SSE Error:  3095.4498342496613\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3847.5847058988056\n",
            "SSE Error for iteration 2 :3523.7995531449924\n",
            "SSE Error for iteration 3 :3348.327103301749\n",
            "SSE Error for iteration 4 :3077.889009247367\n",
            "SSE Error for iteration 5 :3012.7736901772064\n",
            "Final SSE Error:  3012.7736901772064\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3805.257355236627\n",
            "SSE Error for iteration 2 :3356.7377268218193\n",
            "SSE Error for iteration 3 :3108.871599280243\n",
            "SSE Error for iteration 4 :2994.9217578350226\n",
            "Final SSE Error:  2994.9217578350226\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3775.7566628963136\n",
            "SSE Error for iteration 2 :3680.485980960504\n",
            "SSE Error for iteration 3 :3215.148582197313\n",
            "SSE Error for iteration 4 :2962.218233789649\n",
            "Final SSE Error:  2962.218233789649\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3846.7139759559464\n",
            "SSE Error for iteration 2 :3555.332518328419\n",
            "SSE Error for iteration 3 :3176.722173630342\n",
            "SSE Error for iteration 4 :2920.784422837743\n",
            "Final SSE Error:  2920.784422837743\n",
            "\n",
            "Training stage started\n",
            "\n",
            "SSE Error for iteration 1 :3754.349140264509\n",
            "SSE Error for iteration 2 :3553.2875969518855\n",
            "SSE Error for iteration 3 :3312.849610246332\n",
            "SSE Error for iteration 4 :2940.047821611348\n",
            "SSE Error for iteration 5 :2890.0240541581506\n",
            "Final SSE Error:  2890.0240541581506\n",
            "\n",
            "Printing the required output table:\n",
            "\n",
            "    Value_of_K    SSE_Error                                   Size_of_Clusters\n",
            "1            5  3369.793293           {0: 2154, 1: 763, 2: 759, 3: 46, 4: 207}\n",
            "2           10  3204.847165  {0: 1368, 1: 189, 2: 355, 3: 751, 4: 265, 5: 5...\n",
            "3           15  3174.977366  {0: 1102, 1: 336, 2: 720, 3: 238, 4: 91, 5: 18...\n",
            "4           20  3097.840013  {0: 987, 1: 171, 2: 350, 3: 79, 4: 188, 5: 389...\n",
            "5           25  3095.449834  {0: 812, 1: 348, 2: 171, 3: 324, 4: 176, 5: 28...\n",
            "6           30  3012.773690  {0: 862, 1: 181, 2: 414, 3: 100, 4: 208, 5: 13...\n",
            "7           35  2994.921758  {0: 639, 1: 158, 2: 130, 3: 411, 4: 298, 5: 17...\n",
            "8           40  2962.218234  {0: 638, 1: 71, 2: 77, 3: 406, 4: 123, 5: 64, ...\n",
            "9           45  2920.784423  {0: 351, 1: 73, 2: 162, 3: 92, 4: 27, 5: 319, ...\n",
            "10          50  2890.024054  {0: 461, 1: 55, 2: 59, 3: 347, 4: 381, 5: 154,...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3rHpWX0kJ6W9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}